# MSRwPM4Py

This repository contains the code and data sets that were generated and used for my bachelor thesis "Process Discovery and Social Network Analysis with PM4Py".

## datacollection

Contains all python files that were used to collect data from Git and GitHub repositories.

### git_information

Contains the class `GitRepo` that takes the path to a local git repository.

The `get_commit_information` method of this class takes a path as input and will create a csv file of the commit log of the specified local path to the git repo. The columns of this log are:

- commit:hash
- commit:message
- commit:author:name
- commit:author:mail
- commit:committer:name
- commit:committer:mail
- timestamp:author:date
- timestamp:author:timezone
- timestamp:committer:date
- timestamp:committer:timezone
- timestamp:committer
- timestamp:author
- commit:is_merge

### github_information

Contains the class `GitHubRepo` that takes an GitHub authtoken, the organization of the repo and the repo name as input.

The `build_logs` method takes a path to a directory as input where csv files of different logs will be stored. These logs are: 

- General Issue information (repo_issue_info.csv)
- General Pull Request information (repo_pulls_info.csv)
- Log of comments on issues of the repository (repo_issue_comments.csv)
- Log of events of the issues of the repository (repo_issue_events.csv)
- Log combining the comment and event log history containing comments and events of issues (repo_issue_log.csv)
- Log of issue comments, pull request comments, reviews and review comments on pull requests of the repository (repo_pulls_comments.csv)
- Log of events of the Pull Requests of the repository (repo_pulls_events.csv)
- Log combining the comment and event log history containing comments and events of pull requests (repo_pulls_log.csv)
- General information on both Isssues and Pull Requests (repo_information.csv)
- Log combining the complete issue log with the complete pull request log (repo_log.csv)

The included fields for these logs are:

- issue:number
- issue:type
- timestamp
- author:name
- author:id
- author:association
- message
- commit:hash
- activity

The included fields for the information files are:
- issue:number
- issue:title
- issue:labels
- issue:timestamp:opened
- issue:timestamp
- issue:state
- issue:owner:name
- issue:owner:id
- issue:assignees:names
- issue:assignees:ids

These files are created such that it can eb written into them at runtime as it would take too much RAM if all messages / comments are collected before saving the final file.

The `get_username_to_mail_mapping` method of the `GitHubRepo` class creates a csv file that maps the GitHub usernames of an input log to their corresponding mail addresses. The file is stored into repo_user_mappings.csv in the directory that is passed as input. 

#### Used API end points

The used API end points for the Issue Tracking logs were of GitHub's API (https://api.github.com):
- /issues
- /issues/{issue_number}/comments
- /issues/{issue_number}/events
- /pulls/{pull_number}/reviews
- /pulls/comments
- /users/{user}

For more information on how GitHub's API works, see also: https://docs.github.com/en/rest

## examples

Contains the python scripts that call the implemented functions to retrieve the results that are analyzed in the thesis. They can be used for further researches or for analyzing other repositories.

## preprocessing

Contains all functions that were used to format the collected data into processable event logs.
Tht `merge_logs` function takes the Git and GitHub logs of datacollection as input and merges them into one single log.

The used logs should be:
issue_tracking_info (repo_information.csv, generated by github_information.GitHubRepo.build_logs)
user_mapping (repo_user_mappings.csv, generated by github_information.GitHubRepo.build_logs)
issue_tracking_log (repo_log.csv, generated by github_information.GitHubRepo.build_logs)
git_log (generated by git_information.GitRepo.get_commit_information)

The final log that is then generated has these columns:

- issue:number
- timestamp:author:date
- timestamp:committer:date
- commit:message
- commit:author:name
- commit:committer:name
- timestamp:author
- timestamp:committer
- commit:committer:mail
- commit:author:mail
- commit:hash
- activity

It also contains has the `hash_names_and_mails` that applies the md5 hashing algorithm on all names and mail address columns of a log DataFrame.

## process_discovery

Contains the classes and methods that were used to generate process models using the algorithms implemented in PM4Py.

The `algorithms.py` file has classes for all process discovery algorithms of PM4Py. Parameters such as case identifier, originator names id and timestamp id are taken as parameters of the constructor together with the log DataFrame. Each class has an `apply` method for all its variants that are used to apply the algoritm on the log. It returns the data types of petri net, initial marking and final marking from PM4Py and the visualization as .svg-file

The `evaluations.py` contains the functions to compute fitness, precision, simplicity, generalization and soundness. They take the log, the petri net and its initial marking and final marking that are returned by the apply method of the algorithms as input and compute the specific metri. To get all measures in one DataFrame, the `determine_quality` function can be used.

## social_network_analysis

Contains the classes and methods that were used for social network analysis using the algorithms implemented in PM4Py.

The `algorithms.py` file has the classes `Clustering`, `RolesDiscovery` and `OrganizationalMining`.
Clustering can be applied using one of these metrics:

- Handover of Work
- Working Together
- Similar Activities
- Subcontracting

Clustering and Roles Discovery are used to find groups of originators whereas Orgainzational Mining is used to compute the local diagnostics of different groups / originators:

- Group Relative Focus
- Group Relative Stake
- Gruop Coverage
- Group Member Contribution

All classes have an `apply` method that adds a group column to the input log for the `Clustering` and `RolesDiscovery` classes.
For `OrganizationalMining`, it generates four DataFrames, one for each metric, that contains the computed values.

The `evaluation.py` file has multiple functions. `evaluate_group_coverage`,  `evaluate_group_relative_focus`, `evaluate_member_contribution` and `evaluate_group_relative_stake` compute the mean, variance, minimum and maximum of the DataFrame of each metric. `evaluate_numbers` returns a DataFrame with the number of groups, the number of mappings from an originator to groups, the number of groups of size one and of size greater one and the average group size when groups of size one are considered and when they are ignored.